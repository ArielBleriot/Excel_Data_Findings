ğŸŒ©ï¸ AWS Serverless ETL Pipeline

This project demonstrates how to build a scalable, serverless ETL pipeline on AWS to process and analyze weather data efficiently.

ğŸ—ï¸ Architecture Overview

AWS S3 â€“ Stores incoming raw JSON files and processed Parquet files
AWS Lambda â€“ Triggered on new file uploads, flattens JSON using pandas, writes Parquet back to S3
AWS Glue â€“ Catalogs processed data for querying
Amazon Athena â€“ Enables SQL-based analytics on S3-stored data

ğŸ’» Tech Stack

Python: pandas, boto3, pyarrow

AWS Services: S3, Lambda, Glue, Athena

Tools: DBeaver (database visualization & monitoring)

âš¡ Key Features

Converts JSON â†’ Parquet for optimized storage and fast querying

Fully automated pipeline triggered on file upload

Designed with scalability and cost-efficiency in mind

ğŸ› ï¸ What I Learned

Event-driven data processing using AWS Lambda

Schema evolution and data partitioning best practices

Handling Lambda packaging issues with dependencies

âš ï¸ Challenges Faced

URL decoding of S3 object keys from event payload

Configuring IAM policies for secure Lambda execution

Managing Lambda execution time and memory for large datasets

ğŸ“Œ Key Takeaways

This project reinforced my skills in:

Serverless architectures

AWS data services

Automated ETL pipelines

A foundational experience for real-world Data Engineering projects.

ğŸ“‚ Repository Links

Explore this project on GitHub: Airflow1_project ETL

Related project: Customer Churn Prediction

Made with â¤ï¸ by Ariel Bleriot Ndonfak Sapi
