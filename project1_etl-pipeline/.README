AWS Serverless ETL Pipeline
This project demonstrates how to build a scalable, serverless ETL pipeline on AWS.

Architecture:
AWS S3: Stores incoming raw JSON files and processed Parquet files

AWS Lambda: Triggers on new file upload, flattens JSON using pandas, and writes the data in Parquet format back to S3

AWS Glue: Catalogs the processed data for querying

Amazon Athena: Enables SQL-based analytics on data stored in S3

Tech Stack:
Python (pandas, boto3, pyarrow)

AWS Services: S3, Lambda, Glue, Athena

Key Features:
Converts JSON â†’ Parquet for optimized storage and querying

Fully automated pipeline triggered on file upload

Designed with scalability and cost-efficiency in mind

What I Learned:
Event-driven data processing using AWS Lambda

Schema evolution and data partitioning best practices

Handling Lambda packaging issues with dependencies

Challenges:
URL decoding S3 object keys from event payload

Configuring IAM policies for secure Lambda execution

Managing Lambda execution time and memory for large datasets
